{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "hw06_task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FFhvHMA0_oG"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2-kZ__L0Fij"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/SpirinEgor/HSE.machine_learning/main/06.%20This%20Cat%20Does%20Not%20Exist/cat_136.zip\n",
        "!unzip -q cat_136.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG3PGp3KLZoD"
      },
      "source": [
        "# This cat does not exist\n",
        "__Суммарное количество баллов: 10__\n",
        "\n",
        "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
        "\n",
        "__Тема письма: `[ML][MS][HW06] <ФИО>`, где вместо `<ФИО>` указаны фамилия и имя__\n",
        "\n",
        "Цель этого задания - создать котов, которых не существует. В ходе данного задания вы обучите DCGAN и VAE, которые являются одними из первых генеративных моделей. Для этого задания вам наверняка потребуется GPU с CUDA, поэтому рекомендуется использовать Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp0aOFOrLZoJ"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import numpy\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5R6PVjMWUHs"
      },
      "source": [
        "SEED = 7\n",
        "torch.manual_seed(SEED)\n",
        "numpy.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia5Mh4DpLZoK"
      },
      "source": [
        "def random_noise(batch_size, channels, side_size):\n",
        "    return torch.randn(batch_size, channels, side_size, side_size).cuda()\n",
        "\n",
        "def imagewide_average(x):\n",
        "    return x.mean(dim=(-1, -2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzjaQBlSLZoK"
      },
      "source": [
        "def visualise(imgs, rows=2):\n",
        "    imgs = (imgs.transpose(1, 3) + 1) / 2\n",
        "    imgs = torch.cat([imgs[i::rows] for i in range(rows)], dim=1)\n",
        "    cols = len(imgs)\n",
        "    imgs = (torch.cat(list(imgs), dim=1)).cpu().numpy()[:, :, ::-1]\n",
        "    plt.figure(figsize=(cols*1.5, rows*1.5))\n",
        "    plt.imshow(imgs)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY7GFJXgLZoK"
      },
      "source": [
        "class CatDataset(Dataset):\n",
        "    def __init__(self, path_to_dataset=\"cat_136\", size=64):\n",
        "        self.photo_names = os.listdir(path_to_dataset)\n",
        "        self.path_base = path_to_dataset\n",
        "        self.size = size\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        path = self.path_base + \"/\" + self.photo_names[index]\n",
        "        img = cv2.imread(path) # 136 x 136\n",
        "        crop_rate = 8\n",
        "        x_crop = random.randint(0, crop_rate)\n",
        "        y_crop = random.randint(0, crop_rate)\n",
        "        img = img[x_crop:512 - crop_rate + x_crop, y_crop:512 - crop_rate + y_crop]\n",
        "        img = cv2.resize(img, (self.size, self.size), interpolation=cv2.INTER_CUBIC)\n",
        "        return 2 * torch.tensor(img).float().transpose(0, 2) / 255. - 1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.photo_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-obhziTLZoL"
      },
      "source": [
        "dataset = CatDataset()\n",
        "visualise(torch.cat([dataset[i].unsqueeze(0) for i in [3, 15, 182, 592, 394, 2941]], dim=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cwefkXgLZoL"
      },
      "source": [
        "### Задание 1 (2 балла)\n",
        "Для начала реализуем генератор для нашего DCGAN. Предлагается использовать следующую архитектуру:\n",
        "\n",
        "![](https://github.com/SpirinEgor/HSE.machine_learning/blob/main/06.%20This%20Cat%20Does%20Not%20Exist/imgs/DCGAN.png?raw=1)\n",
        "\n",
        "Для ее реализации вам потребуются модули `nn.BatchNorm2d`, `nn.Conv2d`, `nn.ConvTranspose2D`, `nn.ReLU`, а также функция `F.interpolate`.\n",
        "\n",
        "#### Методы\n",
        "* `__init__` - принимает на вход `start_size`, `latent_channels`, `start_channels` и `upsamplings`. Первые два аргумента отвечают за размер случайного шума, из которого в последствии будет сгенерирована картинка. `start_channels` отвечает за то, сколько каналов должно быть в картинке перед тем, как к ней будут применены upsampling блоки. `upsamplings` - это количество upsampling блоков, которые должны быть применены к картинке. В каждом таком локе количество каналов уменьшается в два раза.\n",
        "\n",
        "\n",
        "* `forward` - принимает на вход `batch_size`, генерирует `batch_size` картинок из случайного шума."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_M61VtILZoM"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, start_size=2, latent_channels=32, start_channels=1024, upsamplings=6):\n",
        "        super().__init__()\n",
        "        self._start_size = start_size\n",
        "        self._latent_channels = latent_channels\n",
        "\n",
        "        modules = [nn.Conv2d(latent_channels, start_channels, 1, 1, 0)]\n",
        "        for _ in range(upsamplings):\n",
        "            modules += [\n",
        "                nn.ConvTranspose2d(start_channels, start_channels // 2, 4, 2, 1),\n",
        "                nn.BatchNorm2d(start_channels // 2, affine=False),\n",
        "                nn.ReLU()\n",
        "            ]\n",
        "            start_channels //= 2\n",
        "        modules += [nn.Conv2d(start_channels, 3, 1, 1, 0), nn.Tanh()]\n",
        "        self._generator = nn.Sequential(*modules)\n",
        "        \n",
        "    \n",
        "    def forward(self, batch_size: int):\n",
        "        noise = random_noise(batch_size, self._latent_channels, self._start_size)\n",
        "        # torch.Tensor batch_size x 3 x (start_size * 2**upsamplings) x (start_size * 2**upsamplings)\n",
        "        return self._generator(noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFTyqbAULZoM"
      },
      "source": [
        "### Задание 2 (2 балла)\n",
        "Для начала реализуем дискриминатор для нашего DCGAN. Предлагается использовать следующую архитектуру:\n",
        "\n",
        "![](https://github.com/SpirinEgor/HSE.machine_learning/blob/main/06.%20This%20Cat%20Does%20Not%20Exist/imgs/Disc_DCGAN.png?raw=1)\n",
        "\n",
        "Для ее реализации вам потребуются модули `nn.BatchNorm2d`, `nn.Conv2d`, `nn.ReLU` и `nn.Sigmoid`.\n",
        "\n",
        "#### Методы\n",
        "* `__init__` - принимает на вход `start_channels` и `downsamplings`. `start_channels` определяет количество каналов, которые должны быть в изображении перед применением downsampling блоков.\n",
        "\n",
        "\n",
        "* `forward` - принимает на вход `x` - тензор с картинками. Возвращает вектор с размерностью `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpZpwst8LZoM"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, downsamplings=6, start_channels=8):\n",
        "        super().__init__()\n",
        "        modules = [nn.Conv2d(3, start_channels, 1, 1, 0)]\n",
        "        for _ in range(downsamplings):\n",
        "            modules += [\n",
        "                nn.Conv2d(start_channels, start_channels * 2, 3, 2, 1),\n",
        "                nn.BatchNorm2d(start_channels * 2, affine=False),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ]\n",
        "            start_channels *= 2\n",
        "        modules += [nn.Flatten(), nn.Linear(2048, 1), nn.Sigmoid()]\n",
        "        self._discriminator = nn.Sequential(*modules)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # torch.Tensor batch_size x 1\n",
        "        return self._discriminator(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR-qLTcmLZoM"
      },
      "source": [
        "def train_gan():\n",
        "    generator = Generator()\n",
        "    discriminator = Discriminator()\n",
        "    epochs = 50\n",
        "    visualise_every = 10\n",
        "    batch_size = 8\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "\n",
        "    gen_optim = Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "    disc_optim = Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "\n",
        "    dataset = CatDataset(size=128)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
        "        total_batches = 0\n",
        "        gen_loss_avg = 0\n",
        "        disc_loss_avg = 0\n",
        "\n",
        "        for i, batch in tqdm(enumerate(dataloader), total=(len(dataset) + batch_size) // batch_size):\n",
        "            if len(batch) < batch_size:\n",
        "                continue\n",
        "            total_batches += 1\n",
        "            # Positive update\n",
        "            batch = batch.cuda()\n",
        "            pred = discriminator(batch)\n",
        "            loss = F.binary_cross_entropy(pred, torch.ones_like(pred))\n",
        "            disc_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            disc_optim.step()\n",
        "            disc_loss_avg += loss.item()\n",
        "\n",
        "            # Negative update\n",
        "            batch = generator(batch_size).detach()\n",
        "            pred = discriminator(batch)\n",
        "            loss = F.binary_cross_entropy(pred, torch.zeros_like(pred))\n",
        "            disc_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            disc_optim.step()\n",
        "            disc_loss_avg += loss.item()\n",
        "\n",
        "            # Generator update\n",
        "            batch = generator(batch_size)\n",
        "            pred = discriminator(batch)\n",
        "            loss = F.binary_cross_entropy(pred, torch.ones_like(pred))\n",
        "            gen_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            gen_optim.step()\n",
        "            gen_loss_avg += loss.item()\n",
        "\n",
        "        if (ep + 1) % visualise_every == 0:\n",
        "            with torch.no_grad():\n",
        "                visualise(generator(6), rows=2)\n",
        "\n",
        "        print(f\"Epoch {ep+1} | Discriminator loss: {disc_loss_avg / total_batches} | Generator loss: {gen_loss_avg / total_batches}\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        visualise(generator(6), rows=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TfFhwzx9LZoN"
      },
      "source": [
        "train_gan()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOPUdWezLZoN"
      },
      "source": [
        "### Задание 3 (5 баллов)\n",
        "Теперь посмотрим на другую модель: Variational Autoencoder. В отличии от GAN, в котором генератор пытается себя обмануть дискриминатор, а дискриминатор старается не быть обманутым, VAE решает задачу реконструкции элемента множества X с применением регуляризации в латентном пространстве. \n",
        "\n",
        "Полностью архитектура выглядит так:\n",
        "![](https://github.com/SpirinEgor/HSE.machine_learning/blob/main/06.%20This%20Cat%20Does%20Not%20Exist/imgs/VAE.png?raw=1)\n",
        "\n",
        "Из нее можно выделить две части: Encoder (по изображению возвращает mu и sigma) и Decoder (по случайному шуму восстанавливает изображение). На высоком уровне VAE можно представить так:\n",
        "\n",
        "![](https://github.com/SpirinEgor/HSE.machine_learning/blob/main/06.%20This%20Cat%20Does%20Not%20Exist/imgs/VAE_highlevel.png?raw=1)\n",
        "\n",
        "В данном задании вам необходимо реализовать полную архитектуру VAE.\n",
        "\n",
        "#### Методы\n",
        "* `__init__` - принимает на вход `img_size`, `downsamplings`, `latent_size`, `linear_hidden_size`, `down_channels` и `up_channels`. `img_size` - размер стороны входного изображения. `downsamplings` - количество downsampling (и upsampling) блоков. `latent_size` - размер латентного пространства, в котором в который будет закодирована картинка. `linear_hidden_size` количество нейронов на скрытом слое полносвязной сети в конце encoder'а. Для полносвязной сети decoder'а это число стоит умножить на 2. `down_channels` - количество каналов, в которое будет преобразовано трехцветное изображение перед применением `downsampling` блоков. `up_channels` - количество каналов, которое должно получиться после применения всех upsampling блоков.\n",
        "\n",
        "* `forward` - принимает на вход `x`. Считает распределение $N(\\mu, \\sigma^2)$ и вектор $z \\sim N(\\mu, \\sigma^2)$. Возвращает $x'$ - восстановленную из вектора $z$ картинку и $D_{KL}(N(\\mu, \\sigma^2), N(0, 1)) = 0.5 \\cdot (\\sigma^2 + \\mu^2 - \\log \\sigma^2 - 1)$.\n",
        "\n",
        "* `encode` - принимает на вход `x`. Возвращает вектор из распределения $N(\\mu, \\sigma^2)$.\n",
        "\n",
        "* `decode` - принимает на вход `z`. Возвращает восстановленную по вектору картинку.\n",
        "\n",
        "\n",
        "#### Если хочется улучшить качество\n",
        "https://arxiv.org/pdf/1906.00446.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHJ1OvjnrWYa"
      },
      "source": [
        "class VaeEncoder(nn.Module):\n",
        "    def __init__(self, downsamplings: int, latent_size: int, down_channels: int):\n",
        "        super().__init__()\n",
        "        self._latent_size = latent_size\n",
        "        modules = [nn.Conv2d(3, down_channels, 1, 1, 0)]\n",
        "        for _ in range(downsamplings):\n",
        "            modules += [\n",
        "                nn.Conv2d(down_channels, down_channels * 2, 3, 2, 1),\n",
        "                nn.BatchNorm2d(down_channels * 2, affine=False),\n",
        "                nn.LeakyReLU()\n",
        "            ]\n",
        "            down_channels *= 2\n",
        "        modules += [nn.Conv2d(down_channels, 2 * latent_size, 1, 1, 0), nn.Flatten()]\n",
        "        self._encoder = nn.Sequential(*modules)\n",
        "    \n",
        "    def forward(self, images):\n",
        "        encoded = self._encoder(images)\n",
        "        assert encoded.shape[1] == self._latent_size * 2\n",
        "        mu, log_sigma = torch.split(encoded, self._latent_size, dim=1)\n",
        "        return mu, log_sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJE0PZIx3TNJ"
      },
      "source": [
        "class VaeDecoder(nn.Module):\n",
        "    def __init__(self, upsamplings: int, latent_size: int, up_channels: int):\n",
        "        super().__init__()\n",
        "        start_channels = up_channels * (2 ** upsamplings)\n",
        "        modules = [nn.Conv2d(latent_size, start_channels, 1, 1, 0)]\n",
        "        for _ in range(upsamplings):\n",
        "            modules += [\n",
        "                nn.ConvTranspose2d(start_channels, start_channels // 2, 4, 2, 1),\n",
        "                nn.BatchNorm2d(start_channels // 2, affine=False),\n",
        "                nn.LeakyReLU()\n",
        "            ]\n",
        "            start_channels //= 2\n",
        "        modules += [nn.Conv2d(start_channels, 3, 1, 1, 0), nn.Tanh()]\n",
        "        self._decoder = nn.Sequential(*modules)\n",
        "    \n",
        "    def forward(self, embeddings):\n",
        "        embeddings = embeddings.reshape(*embeddings.shape, 1, 1)\n",
        "        return self._decoder(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfVEsDs7LZoN"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_size=128, latent_size=512, down_channels=8, up_channels=16):\n",
        "        super().__init__()\n",
        "        n_samplings = int(numpy.log2(image_size))\n",
        "        self._encoder = VaeEncoder(n_samplings, latent_size, down_channels)\n",
        "        self._decoder = VaeDecoder(n_samplings, latent_size, up_channels)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        mu, log_sigma = self._encoder(x)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "\n",
        "        kld = 0.5 * (sigma + torch.square(mu) - log_sigma - 1)\n",
        "\n",
        "        z = mu + torch.randn_like(sigma) * sigma\n",
        "        x_pred = self._decoder(z)\n",
        "        return x_pred, kld\n",
        "\n",
        "    def encode(self, x):\n",
        "        mu, log_sigma = self._encoder(x)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "    \n",
        "        return mu + torch.randn_like(sigma) * sigma\n",
        "    \n",
        "    def decode(self, z):\n",
        "        return self._decoder(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "E_uy9QF0LZoO"
      },
      "source": [
        "def train_vae():\n",
        "    vae = VAE()\n",
        "    vae.cuda()\n",
        "\n",
        "    epochs = 100\n",
        "    batch_size = 8\n",
        "    vae_optim = Adam(vae.parameters(), lr=1e-4)\n",
        "\n",
        "    dataset = CatDataset(size=128)\n",
        "\n",
        "    test_imgs_1 = torch.cat([dataset[i].unsqueeze(0) for i in (0, 34, 76, 1509)])\n",
        "    test_imgs_2 = torch.cat([dataset[i].unsqueeze(0) for i in (734, 123, 512, 3634)])\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
        "        total_batches = 0\n",
        "        rec_loss_avg = 0\n",
        "        kld_loss_avg = 0\n",
        "\n",
        "        if ep % 10 == 0:\n",
        "            with torch.no_grad():\n",
        "                z_1 = vae.encode(test_imgs_1.cuda())\n",
        "                z_2 = vae.encode(test_imgs_2.cuda())\n",
        "                x_int = []\n",
        "                for i in range(9):\n",
        "                    z = (i * z_1 + (8 - i) * z_2) / 8\n",
        "                    x_int.append(vae.decode(z))\n",
        "                x_int = torch.cat(x_int)\n",
        "                visualise(x_int, rows=len(test_imgs_1))\n",
        "                z_rand = torch.randn_like(z_1)\n",
        "                x_int = vae.decode(z_rand)\n",
        "                visualise(x_int, rows=len(test_imgs_1)//2)\n",
        "\n",
        "        for i, batch in tqdm(enumerate(dataloader), total=(len(dataset) + batch_size) // batch_size):\n",
        "            if len(batch) < batch_size:\n",
        "                continue\n",
        "            total_batches += 1\n",
        "            x = batch.cuda()\n",
        "            x_rec, kld = vae(x)\n",
        "            img_elems = float(numpy.prod(list(batch.size())))\n",
        "            kld_loss = kld.sum() / batch_size\n",
        "            rec_loss = ((x_rec - x)**2).sum() / batch_size\n",
        "            loss = rec_loss + 0.1 * kld_loss # https://openreview.net/forum?id=Sy2fzU9gl\n",
        "            vae_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            vae_optim.step()\n",
        "            kld_loss_avg += kld_loss.item()\n",
        "            rec_loss_avg += rec_loss.item()\n",
        "\n",
        "        print(f\"Epoch {ep+1} | Reconstruction loss: {rec_loss_avg / total_batches} | KLD loss: {kld_loss_avg / total_batches}\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        z_rand = torch.randn_like(z_1)\n",
        "        x_int = vae.decode(z_rand)\n",
        "        visualise(x_int, rows=len(test_imgs_1)//2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWBbI0pQrPOG"
      },
      "source": [
        "train_vae()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}